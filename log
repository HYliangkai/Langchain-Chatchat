2023-12-07 13:41:09,540 - startup.py[line:646] - INFO: 正在启动服务：
2023-12-07 13:41:09,540 - startup.py[line:647] - INFO: 如需查看 llm_api 日志，请前往 /home/luxinyu/software/Langchain-Chatchat/logs


==============================Langchain-Chatchat Configuration==============================
操作系统：Linux-5.15.0-89-generic-x86_64-with-glibc2.31.
python版本：3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0]
项目版本：v0.2.7
langchain版本：0.0.342. fastchat版本：0.2.32


当前使用的分词器：ChineseRecursiveTextSplitter
当前启动的LLM模型：['chatglm2-6b', 'chatglm3-6b-32k', 'openai-api'] @ cuda
{'device': 'cuda',
 'gpus': '1,2',
 'host': '0.0.0.0',
 'infer_turbo': False,
 'max_gpu_memory': '20GiB',
 'model_path': '/home/luxinyu/models/chatglm2-6b',
 'num_gpus': 2,
 'port': 20002}
{'device': 'cuda',
 'gpus': '1,2',
 'host': '0.0.0.0',
 'infer_turbo': False,
 'max_gpu_memory': '20GiB',
 'model_path': '/home/luxinyu/models/chatglm3-6b-32k',
 'num_gpus': 2,
 'port': 20002}
{'api_base_url': 'https://api.openai.com/v1',
 'api_key': '',
 'device': 'cuda',
 'gpus': '1,2',
 'host': '0.0.0.0',
 'infer_turbo': False,
 'max_gpu_memory': '20GiB',
 'model_name': 'gpt-35-turbo',
 'num_gpus': 2,
 'online_api': True,
 'openai_proxy': '',
 'port': 20002}
当前Embbedings模型： m3e-base @ cuda
==============================Langchain-Chatchat Configuration==============================


2023-12-07 13:41:19 | ERROR | stderr | INFO:     Started server process [17956]
2023-12-07 13:41:19 | ERROR | stderr | INFO:     Waiting for application startup.
2023-12-07 13:41:19 | ERROR | stderr | INFO:     Application startup complete.
2023-12-07 13:41:19 | ERROR | stderr | INFO:     Uvicorn running on http://0.0.0.0:20000 (Press CTRL+C to quit)
2023-12-07 13:41:21 | INFO | model_worker | Loading the model ['chatglm2-6b'] on worker d136e0b6 ...
2023-12-07 13:41:21 | INFO | model_worker | Loading the model ['chatglm3-6b-32k'] on worker e658db12 ...
2023-12-07 13:41:22 | ERROR | stderr | Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]
2023-12-07 13:41:22 | ERROR | stderr | Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]
2023-12-07 13:41:31 | ERROR | stderr | Loading checkpoint shards:  14%|█▍        | 1/7 [00:09<00:58,  9.67s/it]
2023-12-07 13:41:32 | ERROR | stderr | Loading checkpoint shards:  14%|█▍        | 1/7 [00:10<01:02, 10.48s/it]
2023-12-07 13:41:41 | ERROR | stderr | Loading checkpoint shards:  29%|██▊       | 2/7 [00:19<00:48,  9.63s/it]
2023-12-07 13:41:43 | ERROR | stderr | Loading checkpoint shards:  29%|██▊       | 2/7 [00:21<00:53, 10.71s/it]
2023-12-07 13:41:50 | ERROR | stderr | Loading checkpoint shards:  43%|████▎     | 3/7 [00:28<00:37,  9.34s/it]
2023-12-07 13:41:54 | ERROR | stderr | Loading checkpoint shards:  43%|████▎     | 3/7 [00:32<00:42, 10.70s/it]
2023-12-07 13:41:59 | ERROR | stderr | Loading checkpoint shards:  57%|█████▋    | 4/7 [00:37<00:28,  9.37s/it]
2023-12-07 13:42:04 | ERROR | stderr | Loading checkpoint shards:  57%|█████▋    | 4/7 [00:42<00:31, 10.51s/it]
2023-12-07 13:42:09 | ERROR | stderr | Loading checkpoint shards:  71%|███████▏  | 5/7 [00:47<00:19,  9.55s/it]
2023-12-07 13:42:15 | ERROR | stderr | Loading checkpoint shards:  71%|███████▏  | 5/7 [00:52<00:21, 10.56s/it]
2023-12-07 13:42:19 | ERROR | stderr | Loading checkpoint shards:  86%|████████▌ | 6/7 [00:57<00:09,  9.70s/it]
2023-12-07 13:42:25 | ERROR | stderr | Loading checkpoint shards:  86%|████████▌ | 6/7 [01:03<00:10, 10.42s/it]
2023-12-07 13:42:26 | ERROR | stderr | Loading checkpoint shards: 100%|██████████| 7/7 [01:03<00:00,  8.62s/it]
2023-12-07 13:42:26 | ERROR | stderr | Loading checkpoint shards: 100%|██████████| 7/7 [01:03<00:00,  9.13s/it]
2023-12-07 13:42:26 | ERROR | stderr | 
2023-12-07 13:42:26 | INFO | model_worker | Register to controller
2023-12-07 13:42:28 | ERROR | stderr | Loading checkpoint shards: 100%|██████████| 7/7 [01:06<00:00,  8.10s/it]
2023-12-07 13:42:28 | ERROR | stderr | Loading checkpoint shards: 100%|██████████| 7/7 [01:06<00:00,  9.48s/it]
2023-12-07 13:42:28 | ERROR | stderr | 
2023-12-07 13:42:28 | INFO | model_worker | Register to controller
ERROR:    [Errno 98] error while attempting to bind on address ('0.0.0.0', 20002): address already in use
INFO:     Started server process [18377]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:7861 (Press CTRL+C to quit)

Collecting usage statistics. To deactivate, set browser.gatherUsageStats to False.


  You can now view your Streamlit app in your browser.

  URL: http://0.0.0.0:8501

2023-12-07 13:43:09,155 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
{'timeout': 300.0, 'proxies': {'all://127.0.0.1': None, 'all://localhost': None, 'http://127.0.0.1': None, 'http://': None, 'https://': None, 'all://': None}}
INFO:     127.0.0.1:48008 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
2023-12-07 13:43:09,161 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models "HTTP/1.1 200 OK"
2023-12-07 13:43:09,354 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
{'timeout': 300.0, 'proxies': {'all://127.0.0.1': None, 'all://localhost': None, 'http://127.0.0.1': None, 'http://': None, 'https://': None, 'all://': None}}
INFO:     127.0.0.1:48008 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
2023-12-07 13:43:09,357 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:48008 - "POST /llm_model/list_config_models HTTP/1.1" 200 OK
2023-12-07 13:43:09,363 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_config_models "HTTP/1.1 200 OK"
2023-12-07 13:43:09,451 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
{'timeout': 300.0, 'proxies': {'all://127.0.0.1': None, 'all://localhost': None, 'http://127.0.0.1': None, 'http://': None, 'https://': None, 'all://': None}}
INFO:     127.0.0.1:48008 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
2023-12-07 13:43:09,454 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:48008 - "GET /knowledge_base/list_knowledge_bases HTTP/1.1" 200 OK
2023-12-07 13:43:09,490 - _client.py[line:1013] - INFO: HTTP Request: GET http://127.0.0.1:7861/knowledge_base/list_knowledge_bases "HTTP/1.1 200 OK"
2023-12-07 13:43:14,724 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
{'timeout': 300.0, 'proxies': {'all://127.0.0.1': None, 'all://localhost': None, 'http://127.0.0.1': None, 'http://': None, 'https://': None, 'all://': None}}
INFO:     127.0.0.1:48020 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
2023-12-07 13:43:14,728 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models "HTTP/1.1 200 OK"
2023-12-07 13:43:14,918 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
{'timeout': 300.0, 'proxies': {'all://127.0.0.1': None, 'all://localhost': None, 'http://127.0.0.1': None, 'http://': None, 'https://': None, 'all://': None}}
INFO:     127.0.0.1:48020 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
2023-12-07 13:43:14,922 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:48020 - "POST /llm_model/list_config_models HTTP/1.1" 200 OK
2023-12-07 13:43:14,928 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_config_models "HTTP/1.1 200 OK"
2023-12-07 13:43:15,015 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
{'timeout': 300.0, 'proxies': {'all://127.0.0.1': None, 'all://localhost': None, 'http://127.0.0.1': None, 'http://': None, 'https://': None, 'all://': None}}
INFO:     127.0.0.1:48020 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
2023-12-07 13:43:15,019 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:48020 - "GET /knowledge_base/list_knowledge_bases HTTP/1.1" 200 OK
2023-12-07 13:43:15,032 - _client.py[line:1013] - INFO: HTTP Request: GET http://127.0.0.1:7861/knowledge_base/list_knowledge_bases "HTTP/1.1 200 OK"
2023-12-07 13:43:38,265 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
{'timeout': 300.0, 'proxies': {'all://127.0.0.1': None, 'all://localhost': None, 'http://127.0.0.1': None, 'http://': None, 'https://': None, 'all://': None}}
INFO:     127.0.0.1:44130 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
2023-12-07 13:43:38,269 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:44130 - "POST /llm_model/list_config_models HTTP/1.1" 200 OK
2023-12-07 13:43:38,276 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_config_models "HTTP/1.1 200 OK"
2023-12-07 13:43:38,362 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
{'timeout': 300.0, 'proxies': {'all://127.0.0.1': None, 'all://localhost': None, 'http://127.0.0.1': None, 'http://': None, 'https://': None, 'all://': None}}
INFO:     127.0.0.1:44130 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
2023-12-07 13:43:38,365 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:44130 - "GET /knowledge_base/list_knowledge_bases HTTP/1.1" 200 OK
2023-12-07 13:43:38,379 - _client.py[line:1013] - INFO: HTTP Request: GET http://127.0.0.1:7861/knowledge_base/list_knowledge_bases "HTTP/1.1 200 OK"
INFO:     127.0.0.1:44130 - "POST /chat/knowledge_base_chat HTTP/1.1" 200 OK
2023-12-07 13:43:39,096 - SentenceTransformer.py[line:66] - INFO: Load pretrained SentenceTransformer: /home/luxinyu/models/m3e-base
Batches:   0%|          | 0/1 [00:00<?, ?it/s]Batches: 100%|██████████| 1/1 [00:01<00:00,  1.10s/it]Batches: 100%|██████████| 1/1 [00:01<00:00,  1.10s/it]
2023-12-07 13:43:45,633 - faiss_cache.py[line:80] - INFO: loading vector store in 'samples/vector_store/m3e-base' from disk.
2023-12-07 13:43:45,642 - loader.py[line:54] - INFO: Loading faiss with AVX2 support.
2023-12-07 13:43:46,152 - loader.py[line:56] - INFO: Successfully loaded faiss with AVX2 support.
2023-12-07 13:43:46,243 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/chat/knowledge_base_chat "HTTP/1.1 200 OK"
2023-12-07 13:43:46 | INFO | stdout | INFO:     127.0.0.1:60266 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2023-12-07 13:43:46,389 - _client.py[line:1729] - INFO: HTTP Request: POST http://127.0.0.1:20000/v1/chat/completions "HTTP/1.1 200 OK"
2023-12-07 13:43:46 | INFO | httpx | HTTP Request: POST http://127.0.0.1:20002/worker_generate_stream "HTTP/1.1 200 OK"
2023-12-07 17:15:08,560 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
{'timeout': 300.0, 'proxies': {'all://127.0.0.1': None, 'all://localhost': None, 'http://127.0.0.1': None, 'http://': None, 'https://': None, 'all://': None}}
INFO:     127.0.0.1:42808 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
2023-12-07 17:15:08,565 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models "HTTP/1.1 200 OK"
2023-12-07 17:15:08,756 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
{'timeout': 300.0, 'proxies': {'all://127.0.0.1': None, 'all://localhost': None, 'http://127.0.0.1': None, 'http://': None, 'https://': None, 'all://': None}}
INFO:     127.0.0.1:42808 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
2023-12-07 17:15:08,759 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:42808 - "POST /llm_model/list_config_models HTTP/1.1" 200 OK
2023-12-07 17:15:08,764 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_config_models "HTTP/1.1 200 OK"
2023-12-07 17:15:08,849 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
{'timeout': 300.0, 'proxies': {'all://127.0.0.1': None, 'all://localhost': None, 'http://127.0.0.1': None, 'http://': None, 'https://': None, 'all://': None}}
INFO:     127.0.0.1:42808 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
2023-12-07 17:15:08,851 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:42808 - "GET /knowledge_base/list_knowledge_bases HTTP/1.1" 200 OK
2023-12-07 17:15:08,862 - _client.py[line:1013] - INFO: HTTP Request: GET http://127.0.0.1:7861/knowledge_base/list_knowledge_bases "HTTP/1.1 200 OK"
2023-12-07 17:15:24,822 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
{'timeout': 300.0, 'proxies': {'all://127.0.0.1': None, 'all://localhost': None, 'http://127.0.0.1': None, 'http://': None, 'https://': None, 'all://': None}}
INFO:     127.0.0.1:58358 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
2023-12-07 17:15:24,826 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models "HTTP/1.1 200 OK"
2023-12-07 17:15:25,026 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
{'timeout': 300.0, 'proxies': {'all://127.0.0.1': None, 'all://localhost': None, 'http://127.0.0.1': None, 'http://': None, 'https://': None, 'all://': None}}
INFO:     127.0.0.1:58358 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
2023-12-07 17:15:25,030 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:58358 - "POST /llm_model/list_config_models HTTP/1.1" 200 OK
2023-12-07 17:15:25,036 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_config_models "HTTP/1.1 200 OK"
2023-12-07 17:15:25,125 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
{'timeout': 300.0, 'proxies': {'all://127.0.0.1': None, 'all://localhost': None, 'http://127.0.0.1': None, 'http://': None, 'https://': None, 'all://': None}}
INFO:     127.0.0.1:58358 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
2023-12-07 17:15:25,129 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:58358 - "GET /knowledge_base/list_knowledge_bases HTTP/1.1" 200 OK
2023-12-07 17:15:25,143 - _client.py[line:1013] - INFO: HTTP Request: GET http://127.0.0.1:7861/knowledge_base/list_knowledge_bases "HTTP/1.1 200 OK"
2023-12-07 17:16:01,566 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
{'timeout': 300.0, 'proxies': {'all://127.0.0.1': None, 'all://localhost': None, 'http://127.0.0.1': None, 'http://': None, 'https://': None, 'all://': None}}
INFO:     127.0.0.1:53994 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
2023-12-07 17:16:01,570 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models "HTTP/1.1 200 OK"
2023-12-07 17:16:01,761 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
{'timeout': 300.0, 'proxies': {'all://127.0.0.1': None, 'all://localhost': None, 'http://127.0.0.1': None, 'http://': None, 'https://': None, 'all://': None}}
INFO:     127.0.0.1:53994 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
2023-12-07 17:16:01,764 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:53994 - "POST /llm_model/list_config_models HTTP/1.1" 200 OK
2023-12-07 17:16:01,770 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_config_models "HTTP/1.1 200 OK"
2023-12-07 17:16:01,858 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
{'timeout': 300.0, 'proxies': {'all://127.0.0.1': None, 'all://localhost': None, 'http://127.0.0.1': None, 'http://': None, 'https://': None, 'all://': None}}
INFO:     127.0.0.1:53994 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
2023-12-07 17:16:01,861 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:53994 - "GET /knowledge_base/list_knowledge_bases HTTP/1.1" 200 OK
2023-12-07 17:16:01,875 - _client.py[line:1013] - INFO: HTTP Request: GET http://127.0.0.1:7861/knowledge_base/list_knowledge_bases "HTTP/1.1 200 OK"
